{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0728bb-8eac-431f-b94e-c55d868e7c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scikit-multilearn in ./venv/lib/python3.12/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b66ace-ccef-463b-999f-55526966477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/glkaiky/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/glkaiky/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/glkaiky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/glkaiky/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.metrics import classification_report, hamming_loss, accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d08526-eba5-4008-a31a-de3d027c9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('data/train.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: Arquivo 'train.csv' não encontrado. Faça o download do Kaggle e coloque na mesma pasta do notebook.\")\n",
    "    # Cria um dataframe de exemplo para o código não quebrar\n",
    "    df = pd.DataFrame({\n",
    "        'comment_text': ['This is a good, clean comment.', 'This is an obscene, hateful comment.', 'This is an insult.'],\n",
    "        'toxic': [0, 1, 1], 'severe_toxic': [0, 0, 0], 'obscene': [0, 1, 0],\n",
    "        'threat': [0, 0, 0], 'insult': [0, 0, 1], 'identity_hate': [0, 1, 0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec2ddd5-23ee-444d-9d6d-2d6891bfc1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando pré-processamento de texto...\n",
      "Pré-processamento concluído.\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Converte para minúsculas\n",
    "    text = text.lower()\n",
    "    # Remove pontuação e caracteres não-alfabéticos\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokeniza\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words e aplica lematização\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"Iniciando pré-processamento de texto...\")\n",
    "# Aplicando a limpeza em uma amostra para agilizar (remova .sample() para rodar em tudo)\n",
    "# df['clean_comment'] = df['comment_text'].sample(n=10000, random_state=42).apply(clean_text)\n",
    "# df = df.dropna(subset=['clean_comment'])\n",
    "df['clean_comment'] = df['comment_text'].apply(clean_text)\n",
    "print(\"Pré-processamento concluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c10131-8ce1-4c26-a7e6-a10122ee8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_comment']\n",
    "y = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171d8583-7d12-46ad-8339-7d310603af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando vetorização com TF-IDF...\n",
      "Vetorização concluída.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando vetorização com TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000) # Usando as 5000 features mais relevantes\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "print(\"Vetorização concluída.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9625add8-885d-4376-b5c5-724f7f79f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49a89f11-6ef7-4583-88dd-4cd96e44cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando treinamento do modelo...\n",
      "Treinamento concluído.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando treinamento do modelo...\")\n",
    "base_classifier = LogisticRegression(solver='liblinear')\n",
    "classifier = BinaryRelevance(classifier=base_classifier, require_dense=[False, True])\n",
    "classifier.fit(X_train, y_train)\n",
    "print(\"Treinamento concluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0740d970-c80d-4f8a-b940-2aade44bd2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fazendo previsões no conjunto de teste...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fazendo previsões no conjunto de teste...\")\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5cc317-5d6d-4e41-96b4-48fbdee33746",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = hamming_loss(y_test, predictions)\n",
    "subset_accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions, target_names=y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e47fcb14-621e-4664-9920-3de721afbdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados da Avaliação ---\n",
      "Hamming Loss: 0.01907149\n",
      "Acurácia (Subset Accuracy): 0.91988093\n",
      "\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.90      0.62      0.74      3056\n",
      " severe_toxic       0.58      0.26      0.36       321\n",
      "      obscene       0.91      0.64      0.75      1715\n",
      "       threat       0.71      0.14      0.23        74\n",
      "       insult       0.83      0.53      0.65      1614\n",
      "identity_hate       0.73      0.16      0.26       294\n",
      "\n",
      "    micro avg       0.87      0.57      0.69      7074\n",
      "    macro avg       0.78      0.39      0.50      7074\n",
      " weighted avg       0.86      0.57      0.68      7074\n",
      "  samples avg       0.06      0.05      0.05      7074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Resultados da Avaliação ---\")\n",
    "print(f\"Hamming Loss: {loss:.8f}\")\n",
    "print(f\"Acurácia (Subset Accuracy): {subset_accuracy:.8f}\") # Imprime a nova métrica\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b32054-7c4c-41e3-bd34-83903fb00f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
